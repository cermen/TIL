# 머신러닝 5일차

### 앙상블 학습

여러 개의 알고리즘을 사용하여, 그 예측을 결합함으로써 보다 정확한 예측을 도출하는 기법

- 보팅(Voting) : 같은 데이터 세트를 이용하고 서로 다른 분류 알고리즘을 활용해서 최종 예측
  - 보팅 유형 : 하드 보팅(다수결) vs 소프트 보팅(확률)
- 배깅(Bagging) : 데이터 샘플링을 통해서 서브세트를 만들고 같은 분류 알고리즘을 활용해서 최종 예측
  - 배깅 유형 : 랜덤 포레스트(DecisionTreeClassifier) -> 확률



##### Voting 코드

```python
from sklearn.datasets import load_digits

mnist = load_digits()
# mnist.keys()
features , labels = mnist.data , mnist.target

X_train , X_test , y_train , y_test = train_test_split(features , labels , test_size=0.2 , random_state=100)

dt = DecisionTreeClassifier(criterion='entropy' , 
                            max_depth = 8 , 
                            max_features=32 , 
                            random_state=35 ) 
dt.fit(X_train , y_train)
dt_pred = dt.predict(X_test)

knn = KNeighborsClassifier(n_neighbors = 299) 
knn.fit(X_train , y_train)
knn_pred = knn.predict(X_test)

svm = SVC(probability=True) 
svm.fit(X_train , y_train)
svm_pred = svm.predict(X_test)

voting_clf = VotingClassifier(estimators = [ ('tree' , dt) , ('knn' , knn) , ('svm' , svm) ]  , 
                              weights = [1,1,1] , 
                              voting  = 'hard')
voting_clf.fit(X_train , y_train)
hard_voting_pred = voting_clf.predict(X_test)
print('hard voting : ' , accuracy_score(y_test , hard_voting_pred))

voting_clf = VotingClassifier(estimators = [ ('tree' , dt) , ('knn' , knn) , ('svm' , svm) ]  , 
                              weights = [1,1,1] , 
                              voting  = 'soft')
voting_clf.fit(X_train , y_train)
soft_voting_pred = voting_clf.predict(X_test)
print('sotf voting : ' , accuracy_score(y_test , soft_voting_pred ))
```

##### bagging 코드

```python
def user_cross_validation(classifier , features , labels) :
    cv_scores = []
    
    for i in range(10) :
        scores = cross_val_score(classifier , features , labels , cv=10 , scoring='accuracy')
        cv_scores.append( scores.mean() ) 
    
    return cv_scores 

mnist = load_digits()
# mnist.keys()
features , labels = mnist.data , mnist.target

dt_cv_scores = user_cross_validation(DecisionTreeClassifier() , features , labels)
rt_cv_scores = user_cross_validation(RandomForestClassifier() , features , labels)

cv_list = [
    ['decision' , dt_cv_scores],
    ['random forest' , rt_cv_scores]
]

visualDF = pd.DataFrame.from_dict(dict(cv_list))
visualDF.plot()
```

