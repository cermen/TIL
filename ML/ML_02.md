# 머신러닝 2일차

### 교차검증을 간단하게 하는 방법

- 프로세스(폴드 설정 -> 루프반복을 통해서 학습 및 테스트의 인덱스 추출 -> 학습과예측)
- cross_val_score(예측모델 , 피처세트 , 레이블 , 성능평가 지표 , 폴드 수)

```python
from sklearn.tree import DecisionTreeClassifier 
from sklearn.model_selection import cross_val_score , cross_validate
from sklearn.datasets import load_iris

iris_data = load_iris()

data     = iris_data.data
label    = iris_data.target

dt_clf = DecisionTreeClassifier(random_state=100)

# 성능평가지표는 acc , 교차검증 5개
scores = cross_val_score(dt_clf , data , label , cv=5 , scoring='accuracy')

scores = cross_validate(dt_clf , data , label , cv=5 , scoring='accuracy')
```

GridSearchCV : 교차검증과 하이퍼파라미터를 한번에 할 수 있는 툴

- GridSearchCV(분류기, param_grid, 평가방법, refit=True)

```python
from sklearn.tree            import DecisionTreeClassifier
from sklearn.datasets        import load_iris
from sklearn.model_selection import train_test_split , GridSearchCV
from sklearn.metrics         import accuracy_score

import numpy  as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

X_train , X_test , y_train , y_test = train_test_split(iris_data.data , iris_data.target , test_size=0.2, random_state=100)

dt_clf = DecisionTreeClassifier(random_state=100)

grid_parameter = { 'max_depth' : [1,2,3] , 
                   'min_samples_split' : [2 , 3]}
                   
dt_searchCV = GridSearchCV(dt_clf , param_grid = grid_parameter , cv = 3 , refit=True)
dt_searchCV.fit(X_train, y_train)

scoreDF = pd.DataFrame(dt_searchCV.cv_results_)

print('최적의 파라미터  : ' , dt_searchCV.best_params_)
print('최적의 정확도    : ' , dt_searchCV.best_score_)
print('최적의 Estimator : ' , dt_searchCV.best_estimator_)

model = dt_searchCV.best_estimator_
pred  = model.predict(X_test)

print('모델 정확도     : ' , accuracy_score(y_test , pred))
```

## 데이터 전처리

### 데이터 인코딩

- 데이터를 기계가 학습할 수 있는 형태로 변경

#### 라벨인코딩 

- 문자열 -> 숫자로 변환

```python
from sklearn.preprocessing import LabelEncoder

item_label = ['TV' , '냉장고' , '전자렌지' , '컴퓨터' , '선풍기' , '선풍기' , '믹서' , '믹서']

encoder = LabelEncoder()
encoder.fit(item_label)
digit_label = encoder.transform(item_label)
print('encoder : ' , encoder)
print('encoder result : ' , digit_label)
print('*' * 50 )
print('decoder result : ' , encoder.inverse_transform(digit_label))
```

#### One-Hot encoding

- 

```python
from sklearn.preprocessing import OneHotEncoder
import numpy as np 

item_label = ['TV' , '냉장고' , '전자렌지' , '컴퓨터' , '선풍기' , '선풍기' , '믹서' , '믹서']
encoder = LabelEncoder()
encoder.fit(item_label)
digit_label = encoder.transform(item_label)

print('type' , type(digit_label) , digit_label)

# 2차원 데이터 변환
digit_label = digit_label.reshape(-1,1)
print( digit_label.shape  , digit_label) 

# One-Hot 인코딩
one_hot_encoder = OneHotEncoder()
one_hot_encoder.fit(digit_label)
one_hot_label   = one_hot_encoder.transform(digit_label)
print(one_hot_label.shape)
print(one_hot_label.toarray())
```

### 결측치 처리

**missingno**: 결측치를 시각화해 주는 패키지

**설치 방법**

- `conda install -c conda-forge/label/cf202003 missingno`

- `import missingno as ms`

**형식**

- 행렬형 : `msno.matrix(df)`
- 막대형 : `msno.bar(df)`

#### 결측된 데이터를 처리하는 방법

- `df.dropna(thresh=n, axis)` : NaN이 들어있는 행/열 삭제

- SimpleImputer : NaN값 채움
  - strategy : mean , median , most_frequent

```python
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='most_frequent')
df = pd.DataFrame(imputer.fit_transform(df) , 
                  columns=df.columns)
```

#### Feature Scaling

- 표준화(Standardization) : 정규분포(평균이 0 분산 1 가우시안분포 변환) : `StandardScaler()`
  - 공식) (x - x 평균값) / (x 표준편차)
- 정규화(Normalization) : 모든 feature 0 과 1 사이의 값으로변환(음수가 있으면 1) : `MinMaxScaler()`
  - 공식) (x - x 최소값) / (x 최대값- x 최소값)

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler = StandardScaler()
# scaler.fit(iris_df)
# standard_iris = scaler.transform(iris_df)
standard_iris = scaler.fit_transform(iris_df)
# standard_iris

standard_iris_df = pd.DataFrame(data    = standard_iris , 
                                columns = iris.feature_names)
```

