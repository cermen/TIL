# 앙상블 학습

### 개요

- 여러 개의 알고리즘을 사용하여, 그 예측을 결합함으로써 보다 정확한 예측을 도출하는 기법
- 정형 데이터에서 가장 뛰어난 성능을 얻는다.

## 앙상블 학습 알고리즘

### Scikit-learn

#### 랜덤 포레스트

- 결정 트리를 랜덤하게 만들어 결정 트리의 '숲'을 만들고, '부트스트랩 샘플'을 이용하여 각 결정 트리의 예측을 사용해 최종 예측을 만든다.
- 앙상블 학습 알고리즘 중 가장 많이 사용된다.

```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)
```

#### 엑스트라 트리

- 랜덤 포레스트와 비슷하게 결정 트리를 사용하지만, 부트스트랩 샘플을 이용하지 않고 랜덤하게 노드를 분할해 과대적합을 감소시킨다.

```python
from sklearn.ensemble import ExtraTreesClassifier

et = ExtraTreesClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)
```

#### 그레이디언트 부스팅

- 결정 트리를 연속적으로 추가하여 손실 함수를 최소화하는 앙상블 알고리즘

```python
from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(random_state=42)  # 기본값: 깊이 3인 결정트리 100개, 학습률 0.1
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=1)
```

#### 히스토그램 기반 그레이디언트 부스팅

- 기존 그레이디언트 부스팅을 개량한 알고리즘
- 훈련 데이터를 256개 정수 구간으로 나누어 빠르고 높은 성능을 낸다.

```python
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier
hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target, return_train_score=True)
```

### 그 외 알고리즘

#### XGBoost

```python
from xgboost import XGBClassifier
xgb = XGBClassifier(tree_method='hist', random_state=42)
scores = cross_validate(xgb, train_input, train_target, return_train_score=True)
```

#### LightGBM

```python
from lightgbm import LGBMClassifier
lgb = LGBMClassifier(random_state=42)
scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)
```

